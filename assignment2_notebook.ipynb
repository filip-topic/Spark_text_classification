{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b80d5f-9b7b-4635-9014-5d2c2bc0dc9a",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55876edf-0ce5-4e98-a179-63f246b1413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import ChiSqSelector, StandardScaler, Normalizer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441c6266-66c3-44d5-8f91-62fcba31c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Execution mode: 'local' or 'cluster'\n",
    "    'mode': 'cluster',\n",
    "    # Data paths\n",
    "    'local_data_path': './data/reviews_devset.json',\n",
    "    'hdfs_dev_data_path': 'hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json',\n",
    "    'hdfs_full_data_path': 'hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json',\n",
    "    # Output paths\n",
    "    'local_task1_output_path': './output/output_rdd.txt',\n",
    "    'local_task2_output_path': './output/output_ds.txt',\n",
    "    #'hdfs_output_dir': 'hdfs:///user/dic25_shared/output/assignment2',\n",
    "    'hdfs_output_dir': \"hdfs:///user/e12129485/assignment2/output/\",\n",
    "    # Spark config\n",
    "    'spark_master': 'local[*]',  # ignored in cluster mode\n",
    "    'spark_app_name': 'Assignment2_ChiSquare',\n",
    "    # Chi-square and output params\n",
    "    'top_terms_per_category': 75,\n",
    "    'random_seed': 42,\n",
    "    'num_features': 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2de7b32-ad58-4339-b63a-4f200e0a5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task 3\n",
    "config.update({\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio':   0.1,\n",
    "    'test_ratio':  0.1,\n",
    "    'num_folds':   5,\n",
    "    # where to save your model locally\n",
    "    'local_model_output_path': './output/model/',\n",
    "    \"hdfs_model_output_path\": \"hdfs:///user/e12129485/assignment2/output/model/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1743cc-e913-4dd0-8fff-3d91dad82a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Initialize Spark\n",
    "# -------------------------------\n",
    "builder = SparkSession.builder.appName(config['spark_app_name'])\n",
    "if config['mode'] == 'local':\n",
    "    builder = builder.master(config['spark_master'])\n",
    "spark = builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Tune shuffle partitions based on cluster size to avoid tiny/huge shuffles\n",
    "# and set parallelism\n",
    "if config['mode'] == 'local':\n",
    "    # defaultParallelism is usually num cores on your machine\n",
    "    shuffle_partitions = sc.defaultParallelism\n",
    "else:\n",
    "    executor_infos = sc._jsc.sc().statusTracker().getExecutorInfos()\n",
    "    active_executors = [\n",
    "        e.host()\n",
    "        for e in executor_infos\n",
    "        if \"driver\" not in str(e)  # Skip the driver\n",
    "    ]\n",
    "    num_executors = len(active_executors)\n",
    "    executor_cores = int(sc.getConf().get(\"spark.executor.cores\", \"1\"))\n",
    "    shuffle_partitions = num_executors * executor_cores * 4\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions)\n",
    "\n",
    "# -------------------------------\n",
    "# Load Stopwords\n",
    "# -------------------------------\n",
    "\"\"\"with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\"\"\"\n",
    "\n",
    "sc.addFile(\"stopwords.txt\")  # Before using SparkSession\n",
    "stopword_path = SparkFiles.get(\"stopwords.txt\")\n",
    "with open(stopword_path, 'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, lowercase, remove stopwords and single chars\"\"\"\n",
    "    tokens = re.split(r\"[\\s\\d()\\[\\]{}.!?;:+=\\-_'`~#@&*%€$§\\\\/]+\", text.lower())\n",
    "    return [t for t in tokens if t and len(t) > 1 and t not in stopwords]\n",
    "\n",
    "\n",
    "def parse_review(line):\n",
    "    \"\"\"Parse JSON line to (category, reviewText) or None on failure\"\"\"\n",
    "    try:\n",
    "        review = json.loads(line)\n",
    "        return review.get('category'), review.get('reviewText')\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab998f-4ec7-4bac-8c4f-e101107ca04b",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a75604-0873-45ef-92e9-411314a44430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read data\n",
    "data_path = config['local_data_path'] if config['mode'] == 'local' else config['hdfs_dev_data_path']\n",
    "reviews_rdd = sc.textFile(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c70803-4d6b-4b3d-b010-3c2b991fb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Map to ((category, term), count)\n",
    "#    - Map: each review -> [( (cat, term), 1 ) ...]\n",
    "category_term = (\n",
    "    reviews_rdd\n",
    "      .map(parse_review)\n",
    "      .filter(lambda x: x is not None)\n",
    "      .flatMap(lambda ct: [((ct[0], term), 1) for term in preprocess_text(ct[1])])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447945b6-0eb9-4070-8a47-df1c3ebbacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) term_freq: ((cat, term), freq_ct)\n",
    "term_freq = category_term.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0b819b-6430-424c-aca3-a6107c02ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) term_total: (term, freq_t)\n",
    "#    Sum across all categories: column totals for contingency\n",
    "term_total = (\n",
    "    term_freq\n",
    "      .map(lambda kv: (kv[0][1], kv[1]))\n",
    "      .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53e8050-41ac-4dbb-8856-d065a130a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) category_total: (category, total_tokens_in_cat)\n",
    "category_total = (\n",
    "    term_freq\n",
    "      .map(lambda kv: (kv[0][0], kv[1]))\n",
    "      .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c09dbab-5e32-4861-9e28-cba8fdfcd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 6) Grand total N of all tokens\n",
    "N = term_freq.map(lambda kv: kv[1]).sum()\n",
    "\n",
    "# Broadcast small side-data\n",
    "term_total_bc = sc.broadcast(dict(term_total.collect()))\n",
    "category_total_bc = sc.broadcast(dict(category_total.collect()))\n",
    "N_bc = sc.broadcast(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9c235a-f998-41e1-95e9-e0a51ab1ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7) Compute full chi-square per (category, term)\n",
    "#    Using: A = freq_ct; B = freq_t - A; C = cat_total - A; D = N - A - B - C\n",
    "#    chi2 = N * (A*D - B*C)^2 / ((A+B)*(C+D)*(A+C)*(B+D))\n",
    "# -------------------------------\n",
    "def compute_chi(kv):\n",
    "    (cat, term), A = kv\n",
    "    T = term_total_bc.value.get(term, 0)\n",
    "    C_tot = category_total_bc.value.get(cat, 0)\n",
    "    B = T - A\n",
    "    C = C_tot - A\n",
    "    D = N_bc.value - A - B - C\n",
    "    # Avoid zero divisions\n",
    "    denom = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "    if denom == 0:\n",
    "        chi2 = 0.0\n",
    "    else:\n",
    "        chi2 = N_bc.value * (A * D - B * C) ** 2 / denom\n",
    "    return (cat, (term, chi2))\n",
    "\n",
    "chi_sq_rdd = term_freq.map(compute_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d59146-bc5e-485d-b1ab-e1a92575c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8) Top-K terms per category\n",
    "#    Use mapPartitions + heap to pre-aggregate per partition then per key\n",
    "# -------------------------------\n",
    "from heapq import nlargest\n",
    "\n",
    "def topk_per_category(iterator):\n",
    "    # local dict of heaps: cat -> list of (chi2, term)\n",
    "    local = {}\n",
    "    k = config['top_terms_per_category']\n",
    "    for cat, (term, score) in iterator:\n",
    "        if cat not in local:\n",
    "            local[cat] = []\n",
    "        local[cat].append((score, term))\n",
    "    # yield top-K for each cat in this partition\n",
    "    for cat, pairs in local.items():\n",
    "        for score, term in nlargest(k, pairs):\n",
    "            yield (cat, (term, score))\n",
    "\n",
    "# Pre-aggregate in partitions\n",
    "partial_topk = chi_sq_rdd.mapPartitions(topk_per_category)\n",
    "# Final top-K across all partitions\n",
    "top_terms_rdd = (\n",
    "    partial_topk\n",
    "      .groupByKey()\n",
    "      .mapValues(lambda iter_pairs: sorted(iter_pairs, key=lambda x: x[1], reverse=True)[:config['top_terms_per_category']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f116b4cf-ec74-4009-8adb-aa9cddb38f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 9) Merged dictionary: sorted unique terms from chi_sq_rdd\n",
    "# -------------------------------\n",
    "\n",
    "def clean_term(term):\n",
    "    \"\"\"Strip leading/trailing non-alphanumerics (including quotes)\"\"\"\n",
    "    return re.sub(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', term)\n",
    "\n",
    "merged_dict = (\n",
    "    chi_sq_rdd\n",
    "      .map(lambda kv: kv[1][0])\n",
    "      .map(clean_term)\n",
    "      .filter(lambda t: t)\n",
    "      .distinct()\n",
    "      .sortBy(lambda x: x)\n",
    "      .collect()\n",
    ")\n",
    "merged_dict_line = \" \".join(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89bc3c2-1909-441d-9130-d401ea0d8e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 10) Write results\n",
    "#    - Local: driver writes output.txt\n",
    "#    - Cluster/HDFS: use saveAsTextFile\n",
    "# -------------------------------\n",
    "output_lines_rdd = (\n",
    "    top_terms_rdd\n",
    "      .sortByKey()\n",
    "      .map(lambda kv: f\"{kv[0]}\\t\" + \" \".join([f\"{t}:{chi2:.6f}\" for t, chi2 in kv[1]]))\n",
    ")\n",
    "\n",
    "if config['mode'] == 'local':\n",
    "    os.makedirs(os.path.dirname(config['local_task1_output_path']), exist_ok=True)\n",
    "    with open(config['local_task1_output_path'], 'w') as out:\n",
    "        for line in output_lines_rdd.collect():\n",
    "            out.write(line + \"\\n\")\n",
    "        out.write(merged_dict_line + \"\\n\")\n",
    "\n",
    "else:\n",
    "    # Write top terms file\n",
    "    #hdfs_output_path = config['hdfs_output_dir'] + \"output_rdd.txt\"\n",
    "\n",
    "    # Remove existing output directory if it exists\n",
    "    hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(config['hdfs_output_dir'])\n",
    "    fs = path.getFileSystem(hadoop_conf)\n",
    "    \n",
    "    if fs.exists(path):\n",
    "        fs.delete(path, True)  # True for recursive delete\n",
    "        \n",
    "    output_lines_rdd.union(sc.parallelize([merged_dict_line])) \\\n",
    "        .saveAsTextFile(config['hdfs_output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abb2bb-b2cd-4a48-99ba-108d0f8dcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca8c65-83f7-4497-a561-92dcbf389f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92028f0e-b304-4c26-a94a-27eb5f73eb18",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3d03d-7a34-471f-ab1a-459cdeba1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "#data_path = config['local_data_path'] if config['mode'] == 'local' else config['hdfs_dev_data_path']\n",
    "#reviews_df = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96bf7dc-4850-475c-9f2f-31435ae0f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "reviews_df = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "548f22d2-e80d-4bf5-9787-c547f3d8cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"reviewText\",\n",
    "    outputCol=\"tokens\",\n",
    "    pattern=r\"[\\s\\t\\d()\\[\\]{}.!?,;:+=\\-_'`~#@&*%€$§\\\\/]+\",\n",
    "    toLowercase=True,\n",
    "    minTokenLength=2\n",
    ")\n",
    "\n",
    "#remove stopwords   \n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\",\n",
    "    outputCol=\"filtered_tokens\",\n",
    "    stopWords=list(stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb9885e-6e73-4fd4-a729-56375660d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"filtered_tokens\",\n",
    "    outputCol=\"raw_features\",\n",
    "    vocabSize=config['num_features'],\n",
    "    minDF=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7bd7dfc-3030-40fe-98f6-b7ef899fb299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#idf\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "#indexer\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "#pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, indexer])\n",
    "model = pipeline.fit(reviews_df)\n",
    "transformed = model.transform(reviews_df).cache()\n",
    "# materialize cache\n",
    "_ = transformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becf4a39-feb6-45d8-b014-6beb3cfde745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Chi-Square test over all features\n",
    "chi2_result = ChiSquareTest.test(\n",
    "    transformed,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"categoryIndex\"\n",
    ").head()\n",
    "stats = chi2_result.statistics\n",
    "stats_array = stats.toArray() if hasattr(stats, \"toArray\") else stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "897ac1d1-9e01-4538-ae36-97ac388a04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map indices to (term, chi2) and pick top K\n",
    "vocab = model.stages[2].vocabulary\n",
    "indexed = list(enumerate(stats_array))\n",
    "topk = sorted(indexed, key=lambda x: x[1], reverse=True)[:config['num_features']]\n",
    "selected = [(vocab[i], stats_array[i]) for i, _ in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9f1192a-b43c-4b4a-98a7-56c8de03207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out tokens and chi2 scores in descending chi2 order\n",
    "if config['mode'] == 'local':\n",
    "    os.makedirs(os.path.dirname(config['local_task2_output_path']), exist_ok=True)\n",
    "    with open(config['local_task2_output_path'], 'w') as f:\n",
    "        for term, score in selected:\n",
    "            f.write(f\"{term}\\t{score:.6f}\\n\")\n",
    "else:\n",
    "    sc.parallelize([f\"{term}\\t{score:.6f}\" for term, score in selected]) \\\n",
    "      .saveAsTextFile(config['hdfs_output_dir'] + \"output_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2bf50-b76c-4c90-9cd8-d4f1d1ccacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39058-f875-4e07-891e-07252ea603b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f4afa-edf6-4019-b8fd-fea932468718",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8875f3-6fbc-4e2f-b7ef-da5ae1bf5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "#data_path = config['local_data_path'] if config['mode'] == 'local' else config['hdfs_dev_data_path']\n",
    "#reviews_df = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59e63c6-b0fa-4c00-accf-dd03dc541d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) reuse the already-cached TF–IDF DataFrame\n",
    "prepared_df = transformed \\\n",
    "    .select(\"categoryIndex\", \"features\") \\\n",
    "    .cache()\n",
    "# materialize again so the selection is cached\n",
    "_ = prepared_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d454ec-c6df-4438-a60f-0dc3d2a41098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "837ee937-b438-467b-9b9f-3b34b45998a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[categoryIndex: double, features: vector]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Split into train/val/test on the *feature* DataFrame\n",
    "train_df, val_df, test_df = prepared_df.randomSplit(\n",
    "    [config['train_ratio'], config['val_ratio'], config['test_ratio']],\n",
    "    seed=config['random_seed']\n",
    ")\n",
    "train_val_df = train_df.union(val_df).cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a381f120-fcbe-42dc-ab19-1134ae68217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Build *only* the classification pipeline\n",
    "selector = ChiSqSelector(\n",
    "    featuresCol=\"features\",\n",
    "    outputCol=\"selectedFeatures\",\n",
    "    labelCol=\"categoryIndex\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"selectedFeatures\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withMean=False  # keep sparse\n",
    ")\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"scaledFeatures\",\n",
    "    outputCol=\"normFeatures\",\n",
    "    p=2\n",
    ")\n",
    "lsvc = LinearSVC(\n",
    "    featuresCol=\"normFeatures\",\n",
    "    labelCol=\"categoryIndex\"\n",
    ")\n",
    "\n",
    "# Hyper-parameter grid (chi2 top K, standardization, regParam, maxIter)\n",
    "#    See Assignment 2 Part 3 :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(selector.numTopFeatures, [2000])\n",
    "    .addGrid(scaler.withStd, [True, False])\n",
    "    .addGrid(lsvc.regParam, [0.01, 0.1, 1.0])\n",
    "    .addGrid(lsvc.maxIter, [5, 10])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# this is to figure out best parallelisation parameter\n",
    "if config[\"mode\"] == \"local\":\n",
    "    parallelism = 1\n",
    "else:\n",
    "    parallelism = min(len(paramGrid), num_executors * executor_cores)\n",
    "\n",
    "print(f\"Parallelism coefficient: {parallelism}\")\n",
    "\n",
    "ovr = OneVsRest(\n",
    "    classifier=lsvc,\n",
    "    labelCol=\"categoryIndex\",\n",
    "    featuresCol=\"normFeatures\"\n",
    ").setParallelism(parallelism)\n",
    "\n",
    "cls_pipeline = Pipeline(stages=[selector, scaler, normalizer, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d2a522c-8295-4a97-b765-b00406ef10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) evaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"categoryIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da112e64-198d-4e8f-8fde-531bf48669db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/09 20:00:39 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:00:39 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:00:40 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:02:16 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:02:16 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:02:16 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:03:05 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:03:05 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/09 20:03:05 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 5) TrainValidationSplit *on the cached feature DataFrame*\n",
    "\n",
    "# train validation split\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=cls_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    # only classify, so split ratio is train/(train+val)\n",
    "    trainRatio=config['train_ratio'] / (config['train_ratio'] + config['val_ratio']),\n",
    "    parallelism=parallelism,                     \n",
    "    seed=config['random_seed']\n",
    ")\n",
    "\n",
    "# Fit on train+val\n",
    "tvsModel = tvs.fit(train_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0188cac-b28f-4d4a-99ce-fc95b600da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aca1e4-2ee1-4187-8b42-033c004a3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Evaluate on the held‐out test set\n",
    "predictions = tvsModel.transform(test_df)\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(f\"Test F1 score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96919d-a658-4f99-a53b-a9ebd4e8df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) (Optional) persist best model\n",
    "if config['mode']=='local':\n",
    "    tvsModel.bestModel.write().overwrite().save(config['local_model_output_path'])\n",
    "else:\n",
    "    tvsModel.bestModel.write().overwrite().save(os.path.join(config['hdfs_model_output_path'], \"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ad85c-6947-47e7-81cf-21ecb29ad520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Inspect best params\n",
    "sel_m    = tvsModel.bestModel.stages[0]\n",
    "scaler_m = tvsModel.bestModel.stages[1]\n",
    "best_svm = tvsModel.bestModel.stages[-1].getClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f5416-7cea-4eb4-a199-f36f225d3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyper‐parameters:\")\n",
    "print(f\"  numTopFeatures = {sel_m.getNumTopFeatures()}\")\n",
    "print(f\"  withStd        = {scaler_m.getOrDefault('withStd')}\")\n",
    "print(f\"  regParam       = {best_svm.getRegParam()}\")\n",
    "print(f\"  maxIter        = {best_svm.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1533297-8e19-4714-baac-7dccaac1c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
